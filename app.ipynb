{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Fpw2Kmc9xTpZ",
        "0xztpTgy0EMv",
        "EdsF4gxby_Hb",
        "M0DgJqDR1qsh",
        "KnHAwfak0oim",
        "21NSZRfo7frL"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **input**"
      ],
      "metadata": {
        "id": "5Xfl9qeJw7qz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_QC44LBJwx_f"
      },
      "outputs": [],
      "source": [
        "input_video=\"/content/test_lie (1).mp4\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **import**"
      ],
      "metadata": {
        "id": "FLKnq90Tx-jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install mediapipe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4Cz9afiCHZe",
        "outputId": "9dca3f09-f5a8-4575-910c-3daaebc62d39",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.20-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.1.24)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.10.0.84)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.25.6)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Downloading mediapipe-0.10.20-cp311-cp311-manylinux_2_28_x86_64.whl (35.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: sounddevice, mediapipe\n",
            "Successfully installed mediapipe-0.10.20 sounddevice-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import mediapipe as mp\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from moviepy import VideoFileClip\n",
        "import librosa\n",
        "from joblib import load\n",
        "from flask import Flask, request, jsonify\n"
      ],
      "metadata": {
        "id": "3JCn0Drpx0rX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7ceff86-e121-48f9-965a-78016e40c509"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **constrains**"
      ],
      "metadata": {
        "id": "Fpw2Kmc9xTpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check if the input video has no person or has multiple persons\n",
        "# returns True if the input video is correct , false otherwise\n",
        "def input_validation(input_video):\n",
        "    mp_face_detection = mp.solutions.face_detection\n",
        "    face_detection = mp_face_detection.FaceDetection()\n",
        "\n",
        "    cap = cv2.VideoCapture(input_video)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(\"Could not open video\")\n",
        "\n",
        "    valid = True\n",
        "    while cap.isOpened():\n",
        "        ret,frame = cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        results = face_detection.process(rgb_frame)\n",
        "        if not results.detections or len(results.detections) > 1:\n",
        "            valid = False\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    return valid"
      ],
      "metadata": {
        "id": "0nUDYlN5xYE6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_audio_in_video(video_file):\n",
        "    \"\"\"Check if the video contains an audio track.\"\"\"\n",
        "    video = VideoFileClip(video_file)\n",
        "    if video.audio is None:\n",
        "        return 0\n",
        "    return 1"
      ],
      "metadata": {
        "id": "MpTBNIGBxcZY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **models**"
      ],
      "metadata": {
        "id": "0xztpTgy0EMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  saved_model=load_model(\"/content/facial_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XErIrGXmCfNx",
        "outputId": "dc4cc37e-154a-4cf0-b3da-bc51556360f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model=load_model(\"/content/audio_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH6ygbxbCn0P",
        "outputId": "3f2b144c-5bed-4614-bf4c-52a6e17de914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saved_model = load_model(\"C:\\\\Users\\\\MSIQ\\\\Downloads\\\\deployment\\\\facial_model.h5\")"
      ],
      "metadata": {
        "id": "_w2P46Nl0F9F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "001ef285-0ae2-416d-af60-b89403021bdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saved_model=load_model(\"/content/lstm_without_masking(94).h5\")"
      ],
      "metadata": {
        "id": "poJbaJjoQZa-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "403791cc-c9eb-4130-ce99-53fe893dee1f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model= r\"C:\\Users\\MSIQ\\Downloads\\deployment\\lstm_without_masking(94).h5\"\n",
        "# \"C:\\Users\\MSIQ\\Downloads\\deployment\\lstm_without_masking(94).h5\""
      ],
      "metadata": {
        "id": "ZDf9ijiFQJma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = load_model(\"C:\\\\Users\\\\MSIQ\\\\Downloads\\\\deployment\\\\audio_model.h5\")"
      ],
      "metadata": {
        "id": "qWjgVluT0Nd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a57d8a7-4419-4c9a-8703-c2fa5d42edf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Facial Expression**\n"
      ],
      "metadata": {
        "id": "EdsF4gxby_Hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mp_face_detection = mp.solutions.face_detection"
      ],
      "metadata": {
        "id": "dRrW7w9JykJQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_face_from_frame(frame):\n",
        "    height, width, _ = frame.shape\n",
        "    with mp_face_detection.FaceDetection(min_detection_confidence=0.5) as face_detection:\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        results = face_detection.process(rgb_frame)\n",
        "\n",
        "        if results.detections:\n",
        "            detection = results.detections[0]\n",
        "            bboxC = detection.location_data.relative_bounding_box\n",
        "            x = int(bboxC.xmin * width)\n",
        "            y = int(bboxC.ymin * height)\n",
        "            w = int(bboxC.width * width)\n",
        "            h = int(bboxC.height * height)\n",
        "\n",
        "            padding = 0.2\n",
        "            pad_x = int(padding * w)\n",
        "            pad_y = int(padding * h)\n",
        "            x = max(0, x - pad_x)\n",
        "            y = max(0, y - pad_y)\n",
        "            w = min(width - x, w + 2 * pad_x)\n",
        "            h = min(height - y, h + 2 * pad_y)\n",
        "\n",
        "            cropped_face = frame[y:y+h, x:x+w]\n",
        "            cropped_face_resized = cv2.resize(cropped_face, (width, height))\n",
        "            return cropped_face_resized\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "uNqX3PD2zlEL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(\n",
        "        static_image_mode=False,  # to tell the model it is a video not image\n",
        "        max_num_faces=1,   # number of faces the model should detect\n",
        "        min_detection_confidence=0.5,\n",
        "        min_tracking_confidence=0.5,\n",
        "        refine_landmarks=True  # add points for gaze tracking\n",
        "    )"
      ],
      "metadata": {
        "id": "D7qLn_o-znSF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mediapipe_predictions(frame,model):\n",
        "  rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "  results = face_mesh.process(frame)\n",
        "  return results"
      ],
      "metadata": {
        "id": "yuYazvehzpfQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_frame_points(face_landmarks):\n",
        "  frame_points=[]\n",
        "  for landmark in face_landmarks.landmark:\n",
        "    x = landmark.x\n",
        "    y = landmark.y\n",
        "    z = landmark.z\n",
        "    frame_points.append(x)\n",
        "    frame_points.append(y)\n",
        "    frame_points.append(z)\n",
        "\n",
        "  return frame_points"
      ],
      "metadata": {
        "id": "bsNJA1Z2zrUx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_facemesh(frame):\n",
        "  results=mediapipe_predictions(frame,face_mesh)\n",
        "  if results.multi_face_landmarks:\n",
        "    for face_landmarks in results.multi_face_landmarks:\n",
        "      frame_points=extract_frame_points(face_landmarks)\n",
        "  else:\n",
        "    frame_points=np.zeros(478*3)\n",
        "  return frame_points"
      ],
      "metadata": {
        "id": "gaGZno8Vzs-8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels=['truth','lie']"
      ],
      "metadata": {
        "id": "6rCch6Q3zuwY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# def make_prediction(input_video):\n",
        "#     # Open the video capture\n",
        "#     video_capture = cv2.VideoCapture(input_video)\n",
        "#     all_video_predictions = []\n",
        "\n",
        "#     # Check if video file was opened successfully\n",
        "#     if not video_capture.isOpened():\n",
        "#         return 'Could not open video'\n",
        "\n",
        "#     # Validate the input video (ensuring only one person and clear face visibility)\n",
        "#     valid_input = input_validation(input_video)\n",
        "#     if not valid_input:\n",
        "#         return \"The input video should have only one person with a clearly visible face\"\n",
        "\n",
        "#     frame_count = 0\n",
        "#     cur_video32 = []\n",
        "\n",
        "#     while True:\n",
        "#         ret, frame = video_capture.read()\n",
        "#         if not ret:\n",
        "#             break\n",
        "\n",
        "#         frame_count += 1\n",
        "\n",
        "#         # Crop and process face from the current frame\n",
        "#         cropped_frame = crop_face_from_frame(frame)\n",
        "#         if cropped_frame is None:\n",
        "#             cur_video32.append(np.zeros(478 * 3))  # Add zeros if no face detected\n",
        "#         else:\n",
        "#             frame_points = apply_facemesh(cropped_frame)  # Get facial landmarks\n",
        "#             cur_video32.append(frame_points)\n",
        "\n",
        "#         # Process 32 frames before making a prediction\n",
        "#         if frame_count == 32:\n",
        "#             input_data = np.array([cur_video32])  # Create input array for the model\n",
        "#             if saved_model is not None:\n",
        "#                 prediction = saved_model.predict(input_data)  # Get the prediction\n",
        "#                 all_video_predictions.append(np.argmax(prediction,axis=1))\n",
        "#             else:\n",
        "#                 return 'Model is not loaded properly.'\n",
        "#             cur_video32 = []  # Reset for the next 32 frames\n",
        "#             frame_count = 0\n",
        "\n",
        "#     # Handle the last frames if less than 32\n",
        "#     if frame_count != 0:\n",
        "#         while frame_count < 32:\n",
        "#             cur_video32.append(np.zeros(478 * 3))  # Zero-padding\n",
        "#             frame_count += 1\n",
        "#         input_data = np.array([cur_video32])  # Create input array for the model\n",
        "#         if saved_model is not None:\n",
        "#             prediction = saved_model.predict(input_data)\n",
        "#             all_video_predictions.append(np.argmax(prediction,axis=1))\n",
        "#         else:\n",
        "#             return 'Model is not loaded properly.'\n",
        "\n",
        "#     return all_video_predictions\n"
      ],
      "metadata": {
        "id": "18rU9A6N87tT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x=make_prediction(input_video)"
      ],
      "metadata": {
        "id": "tATys2ROSOKk",
        "collapsed": true
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x"
      ],
      "metadata": {
        "id": "aGDzV2b9Sse5",
        "collapsed": true
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9EJ5M4z4TVCZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predictions_combined = np.vstack(x)\n",
        "# facial_predictions = np.argmax(predictions_combined, axis=1)"
      ],
      "metadata": {
        "id": "fTpaHxcYS07H"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# facial_predictions"
      ],
      "metadata": {
        "id": "rvK7dRFuTJ53"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def make_prediction(input_video):\n",
        "#     global saved_model\n",
        "\n",
        "#     # Open video capture\n",
        "#     video_capture = cv2.VideoCapture(input_video)\n",
        "#     all_video_predictions = []\n",
        "\n",
        "#     if not video_capture.isOpened():\n",
        "#         return {\"error\": \"Could not open video\"}\n",
        "\n",
        "#     valid_input = input_validation(input_video)\n",
        "#     if not valid_input:\n",
        "#         return {\"error\": \"The input video should have only one person with a clearly visible face\"}\n",
        "\n",
        "#     frame_count = 0\n",
        "#     cur_video32 = []\n",
        "\n",
        "#     while True:\n",
        "#         ret, frame = video_capture.read()\n",
        "#         if not ret:\n",
        "#             break\n",
        "\n",
        "#         frame_count += 1\n",
        "\n",
        "#         cropped_frame = crop_face_from_frame(frame)\n",
        "#         if cropped_frame is None:\n",
        "#             cur_video32.append(np.zeros(478 * 3))  # Add zeros if no face detected\n",
        "#         else:\n",
        "#             frame_points = apply_facemesh(cropped_frame)\n",
        "#             cur_video32.append(frame_points)\n",
        "\n",
        "#         if frame_count == 32:\n",
        "#             input_data = np.array([cur_video32])\n",
        "#             prediction = saved_model.predict(input_data) if saved_model else [0]  # Dummy prediction\n",
        "\n",
        "\n",
        "\n",
        "#             all_video_predictions.append(np.argmax(prediction))\n",
        "\n",
        "\n",
        "#             cur_video32 = []\n",
        "#             frame_count = 0\n",
        "\n",
        "#     # Handle remaining frames\n",
        "#     if frame_count != 0:\n",
        "#         while frame_count < 32:\n",
        "#             cur_video32.append(np.zeros(478 * 3))\n",
        "#             frame_count += 1\n",
        "#         input_data = np.array([cur_video32])\n",
        "#         prediction = saved_model.predict(input_data) if saved_model else [0]  # Dummy prediction\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#         all_video_predictions.append(np.argmax(prediction))\n",
        "\n",
        "\n",
        "#     return {\"predictions\": all_video_predictions}"
      ],
      "metadata": {
        "id": "9eQ6Vu2KYdoE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x=make_prediction(input_video)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "O6PnPGgHYgD0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ETADRLcOYibY"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Set upload folder\n",
        "UPLOAD_FOLDER = \"uploads\"\n",
        "os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
        "app.config[\"UPLOAD_FOLDER\"] = UPLOAD_FOLDER\n",
        "\n",
        "latest_result = None\n",
        "\n",
        "# Dummy functions (replace these with your actual implementations)\n",
        "def input_validation(video_path):\n",
        "    return True  # Placeholder for actual validation logic\n",
        "\n",
        "def crop_face_from_frame(frame):\n",
        "    return frame  # Placeholder function\n",
        "\n",
        "def apply_facemesh(frame):\n",
        "    return np.random.rand(478 * 3)  # Simulate face mesh extraction\n",
        "\n",
        "# Load your ML model here\n",
        "saved_model = None  # Replace with actual model loading\n",
        "\n",
        "def make_prediction(input_video):\n",
        "    global saved_model\n",
        "\n",
        "    # Open video capture\n",
        "    video_capture = cv2.VideoCapture(input_video)\n",
        "    all_video_predictions = []\n",
        "\n",
        "    if not video_capture.isOpened():\n",
        "        return {\"error\": \"Could not open video\"}\n",
        "\n",
        "    valid_input = input_validation(input_video)\n",
        "    if not valid_input:\n",
        "        return {\"error\": \"The input video should have only one person with a clearly visible face\"}\n",
        "\n",
        "    frame_count = 0\n",
        "    cur_video32 = []\n",
        "\n",
        "    while True:\n",
        "        ret, frame = video_capture.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "        cropped_frame = crop_face_from_frame(frame)\n",
        "        if cropped_frame is None:\n",
        "            cur_video32.append(np.zeros(478 * 3))  # Add zeros if no face detected\n",
        "        else:\n",
        "            frame_points = apply_facemesh(cropped_frame)\n",
        "            cur_video32.append(frame_points)\n",
        "\n",
        "        if frame_count == 32:\n",
        "            input_data = np.array([cur_video32])\n",
        "            prediction = saved_model.predict(input_data) if saved_model else [0]  # Dummy prediction\n",
        "\n",
        "            # Convert prediction to a native Python type if needed\n",
        "            all_video_predictions.append(int(np.argmax(prediction)))  # Convert to int\n",
        "\n",
        "            cur_video32 = []\n",
        "            frame_count = 0\n",
        "\n",
        "    # Handle remaining frames\n",
        "    if frame_count != 0:\n",
        "        while frame_count < 32:\n",
        "            cur_video32.append(np.zeros(478 * 3))\n",
        "            frame_count += 1\n",
        "        input_data = np.array([cur_video32])\n",
        "        prediction = saved_model.predict(input_data) if saved_model else [0]  # Dummy prediction\n",
        "\n",
        "        # Convert prediction to a native Python type if needed\n",
        "        all_video_predictions.append(int(np.argmax(prediction)))  # Convert to int\n",
        "\n",
        "    return {\"predictions\": all_video_predictions}\n",
        "\n",
        "\n",
        "@app.route(\"/analyze\", methods=[\"POST\"])\n",
        "def analyze():\n",
        "    global latest_result\n",
        "\n",
        "    if \"video\" not in request.files:\n",
        "        return jsonify({\"error\": \"No video file provided\"}), 400\n",
        "\n",
        "    video_file = request.files[\"video\"]\n",
        "\n",
        "    if video_file.filename == \"\":\n",
        "        return jsonify({\"error\": \"No selected file\"}), 400\n",
        "\n",
        "    file_path = os.path.join(app.config[\"UPLOAD_FOLDER\"], video_file.filename)\n",
        "    video_file.save(file_path)\n",
        "\n",
        "    result = make_prediction(file_path)\n",
        "    latest_result = result  # Store latest result\n",
        "\n",
        "    return jsonify(result)\n",
        "\n",
        "\n",
        "@app.route(\"/result\", methods=[\"GET\"])\n",
        "def get_result():\n",
        "    if latest_result is None:\n",
        "        return jsonify({\"error\": \"No analysis has been performed yet\"}), 404\n",
        "    return jsonify(latest_result)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(host=\"0.0.0.0\", port=5000, debug=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYreXmZ0b6w_",
        "outputId": "a936a7d6-e478-47db-cfe0-bbf438b56c74"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug: * Restarting with stat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def make_prediction(input_video):\n",
        "#     global saved_model\n",
        "\n",
        "#     # Open video capture\n",
        "#     video_capture = cv2.VideoCapture(input_video)\n",
        "#     all_video_predictions = []\n",
        "\n",
        "#     if not video_capture.isOpened():\n",
        "#         return {\"error\": \"Could not open video\"}\n",
        "\n",
        "#     valid_input = input_validation(input_video)\n",
        "#     if not valid_input:\n",
        "#         return {\"error\": \"The input video should have only one person with a clearly visible face\"}\n",
        "\n",
        "#     frame_count = 0\n",
        "#     cur_video32 = []\n",
        "\n",
        "#     while True:\n",
        "#         ret, frame = video_capture.read()\n",
        "#         if not ret:\n",
        "#             break\n",
        "\n",
        "#         frame_count += 1\n",
        "\n",
        "#         cropped_frame = crop_face_from_frame(frame)\n",
        "#         if cropped_frame is None:\n",
        "#             cur_video32.append(np.zeros(478 * 3))  # Add zeros if no face detected\n",
        "#         else:\n",
        "#             frame_points = apply_facemesh(cropped_frame)\n",
        "#             cur_video32.append(frame_points)\n",
        "\n",
        "#         if frame_count == 32:\n",
        "#             input_data = np.array([cur_video32])\n",
        "#             prediction = saved_model.predict(input_data) if saved_model else [0]  # Dummy prediction\n",
        "\n",
        "#             # Convert prediction to a native Python type if needed\n",
        "#             all_video_predictions.append((np.argmax(prediction)))  # Convert to int\n",
        "\n",
        "#             cur_video32 = []\n",
        "#             frame_count = 0\n",
        "\n",
        "#     # Handle remaining frames\n",
        "#     if frame_count != 0:\n",
        "#         while frame_count < 32:\n",
        "#             cur_video32.append(np.zeros(478 * 3))\n",
        "#             frame_count += 1\n",
        "#         input_data = np.array([cur_video32])\n",
        "#         prediction = saved_model.predict(input_data) if saved_model else [0]  # Dummy prediction\n",
        "\n",
        "#         # Convert prediction to a native Python type if needed\n",
        "#         all_video_predictions.append((np.argmax(prediction)))  # Convert to int\n",
        "\n",
        "#     return {\"predictions\": all_video_predictions}"
      ],
      "metadata": {
        "id": "76Y--MP7jRi6"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x=make_prediction(input_video)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKzAlzhijTAF",
        "outputId": "55a387b4-6d7d-4ba6-dfc7-a4f79be6b32d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 574ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38WuCnbUjVTb",
        "outputId": "b00749a5-f18e-4e0f-d43a-19289b1c4f21"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'predictions': [1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1]}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# def make_prediction(input_video):\n",
        "#     global saved_model\n",
        "\n",
        "#     # Open video capture\n",
        "#     video_capture = cv2.VideoCapture(input_video)\n",
        "#     all_video_predictions = []\n",
        "\n",
        "#     if not video_capture.isOpened():\n",
        "#         return {\"error\": \"Could not open video\"}\n",
        "\n",
        "#     valid_input = input_validation(input_video)\n",
        "#     if not valid_input:\n",
        "#         return {\"error\": \"The input video should have only one person with a clearly visible face\"}\n",
        "\n",
        "#     frame_count = 0\n",
        "#     cur_video32 = []\n",
        "\n",
        "#     while True:\n",
        "#         ret, frame = video_capture.read()\n",
        "#         if not ret:\n",
        "#             break\n",
        "\n",
        "#         frame_count += 1\n",
        "\n",
        "#         cropped_frame = crop_face_from_frame(frame)\n",
        "#         if cropped_frame is None:\n",
        "#             cur_video32.append(np.zeros(478 * 3))  # Add zeros if no face detected\n",
        "#         else:\n",
        "#             frame_points = apply_facemesh(cropped_frame)\n",
        "#             cur_video32.append(frame_points)\n",
        "\n",
        "#         if frame_count == 32:\n",
        "#             input_data = np.array([cur_video32])\n",
        "#             prediction = saved_model.predict(input_data) if saved_model else [0]  # Dummy prediction\n",
        "\n",
        "#             # Convert prediction to a native Python type if needed\n",
        "#             all_video_predictions.append((np.argmax(prediction)))  # Convert to int\n",
        "\n",
        "#             cur_video32 = []\n",
        "#             frame_count = 0\n",
        "\n",
        "#     # Handle remaining frames\n",
        "#     if frame_count != 0:\n",
        "#         while frame_count < 32:\n",
        "#             cur_video32.append(np.zeros(478 * 3))\n",
        "#             frame_count += 1\n",
        "#         input_data = np.array([cur_video32])\n",
        "#         prediction = saved_model.predict(input_data) if saved_model else [0]  # Dummy prediction\n",
        "\n",
        "#         # Convert prediction to a native Python type if needed\n",
        "#         all_video_predictions.append((np.argmax(prediction)))  # Convert to int\n",
        "\n",
        "#     return {\"predictions\": all_video_predictions}\n"
      ],
      "metadata": {
        "id": "q9saMf20dp7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x=make_prediction(input_video)\n"
      ],
      "metadata": {
        "id": "yw0XvPPudrbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x"
      ],
      "metadata": {
        "id": "x-cBpBoFdxuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "\n",
        "# from flask import Flask, request, jsonify\n",
        "# import numpy as np\n",
        "# import os\n",
        "# import cv2\n",
        "\n",
        "# app = Flask(__name__)\n",
        "\n",
        "# # Set upload folder\n",
        "# UPLOAD_FOLDER = \"uploads\"\n",
        "# os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
        "# app.config[\"UPLOAD_FOLDER\"] = UPLOAD_FOLDER\n",
        "\n",
        "# latest_result = None\n",
        "\n",
        "# # Dummy functions (replace these with your actual implementations)\n",
        "# def input_validation(video_path):\n",
        "#     return True  # Placeholder for actual validation logic\n",
        "\n",
        "# def crop_face_from_frame(frame):\n",
        "#     return frame  # Placeholder function\n",
        "\n",
        "# def apply_facemesh(frame):\n",
        "#     return np.random.rand(478 * 3)  # Simulate face mesh extraction\n",
        "\n",
        "# # Load your ML model here\n",
        "# saved_model = None  # Replace with actual model loading\n",
        "\n",
        "# def make_prediction(input_video):\n",
        "#     global saved_model\n",
        "\n",
        "#     # Open video capture\n",
        "#     video_capture = cv2.VideoCapture(input_video)\n",
        "#     all_video_predictions = []\n",
        "\n",
        "#     if not video_capture.isOpened():\n",
        "#         return {\"error\": \"Could not open video\"}\n",
        "\n",
        "#     valid_input = input_validation(input_video)\n",
        "#     if not valid_input:\n",
        "#         return {\"error\": \"The input video should have only one person with a clearly visible face\"}\n",
        "\n",
        "#     frame_count = 0\n",
        "#     cur_video32 = []\n",
        "\n",
        "#     while True:\n",
        "#         ret, frame = video_capture.read()\n",
        "#         if not ret:\n",
        "#             break\n",
        "\n",
        "#         frame_count += 1\n",
        "\n",
        "#         cropped_frame = crop_face_from_frame(frame)\n",
        "#         if cropped_frame is None:\n",
        "#             cur_video32.append(np.zeros(478 * 3))  # Add zeros if no face detected\n",
        "#         else:\n",
        "#             frame_points = apply_facemesh(cropped_frame)\n",
        "#             cur_video32.append(frame_points)\n",
        "\n",
        "#         if frame_count == 32:\n",
        "#             input_data = np.array([cur_video32])\n",
        "#             prediction = saved_model.predict(input_data) if saved_model else [0]\n",
        "\n",
        "\n",
        "#             all_video_predictions.append(np.argmax(prediction))\n",
        "\n",
        "\n",
        "#             cur_video32 = []\n",
        "#             frame_count = 0\n",
        "\n",
        "#     # Handle remaining frames\n",
        "#     if frame_count != 0:\n",
        "#         while frame_count < 32:\n",
        "#             cur_video32.append(np.zeros(478 * 3))\n",
        "#             frame_count += 1\n",
        "#         input_data = np.array([cur_video32])\n",
        "#         prediction = saved_model.predict(input_data) if saved_model else [0]\n",
        "\n",
        "\n",
        "\n",
        "#         all_video_predictions.append(np.argmax(prediction))\n",
        "\n",
        "\n",
        "#     return {\"predictions\": all_video_predictions}\n",
        "\n",
        "# @app.route(\"/analyze\", methods=[\"POST\"])\n",
        "# def analyze():\n",
        "#     global latest_result\n",
        "\n",
        "#     if \"video\" not in request.files:\n",
        "#         return jsonify({\"error\": \"No video file provided\"}), 400\n",
        "\n",
        "#     video_file = request.files[\"video\"]\n",
        "\n",
        "#     if video_file.filename == \"\":\n",
        "#         return jsonify({\"error\": \"No selected file\"}), 400\n",
        "\n",
        "#     file_path = os.path.join(app.config[\"UPLOAD_FOLDER\"], video_file.filename)\n",
        "#     video_file.save(file_path)\n",
        "\n",
        "#     result = make_prediction(file_path)\n",
        "#     latest_result = result  # Store latest result\n",
        "\n",
        "#     return jsonify(result)\n",
        "\n",
        "\n",
        "# @app.route(\"/result\", methods=[\"GET\"])\n",
        "# def get_result():\n",
        "#     if latest_result is None:\n",
        "#         return jsonify({\"error\": \"No analysis has been performed yet\"}), 404\n",
        "#     return jsonify(latest_result)\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     app.run(host=\"0.0.0.0\", port=5000, debug=True)\n"
      ],
      "metadata": {
        "id": "5i9FzOV-OG5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2LwNKCxKWCku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zzk34jVkWCHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **audio**"
      ],
      "metadata": {
        "id": "M0DgJqDR1qsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_audio_from_video(video_file, audio_file):\n",
        "    video = VideoFileClip(video_file)\n",
        "    audio = video.audio\n",
        "    audio.write_audiofile(audio_file, codec='pcm_s16le')\n"
      ],
      "metadata": {
        "id": "JpTnsC4M1lQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "def split_and_pad_audio(audio_file, segment_duration=10, sample_rate=16000):\n",
        "    segment_samples = segment_duration * sample_rate\n",
        "\n",
        "    audio, sr = librosa.load(audio_file, sr=sample_rate)\n",
        "\n",
        "    num_segments = len(audio) // segment_samples\n",
        "    remainder = len(audio) % segment_samples\n",
        "\n",
        "    segments = []\n",
        "    for i in range(num_segments):\n",
        "        segment = audio[i * segment_samples : (i + 1) * segment_samples]\n",
        "        segments.append(segment)\n",
        "\n",
        "    if remainder > 0:\n",
        "        last_segment = audio[num_segments * segment_samples:]\n",
        "        padded_segment = np.pad(last_segment, (0, segment_samples - len(last_segment)), mode='constant')\n",
        "        segments.append(padded_segment)\n",
        "\n",
        "    return segments\n"
      ],
      "metadata": {
        "id": "6TguSCWp1_Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def extract_mfcc_from_segments(segments, sample_rate=16000, n_mfcc=13):\n",
        "    mfcc_features = []\n",
        "\n",
        "    for idx, segment in enumerate(segments):\n",
        "        mfcc = librosa.feature.mfcc(y=segment, sr=sample_rate, n_mfcc=n_mfcc)\n",
        "        mfcc_mean = np.mean(mfcc, axis=1)\n",
        "        mfcc_std = np.std(mfcc, axis=1)\n",
        "\n",
        "        feature_entry = {\n",
        "            \"segment_index\": idx + 1,\n",
        "        }\n",
        "        for i in range(n_mfcc):\n",
        "            feature_entry[f\"mfcc{i+1}_mean\"] = mfcc_mean[i]\n",
        "            feature_entry[f\"mfcc{i+1}_std\"] = mfcc_std[i]\n",
        "\n",
        "        mfcc_features.append(feature_entry)\n",
        "\n",
        "    return pd.DataFrame(mfcc_features)\n",
        "\n"
      ],
      "metadata": {
        "id": "AFZ33z7y2E7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **facial expression predictions**"
      ],
      "metadata": {
        "id": "KnHAwfak0oim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **audio predictions**"
      ],
      "metadata": {
        "id": "21NSZRfo7frL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prediction_audio(input_video):\n",
        "  video_capture = cv2.VideoCapture(input_video)\n",
        "  all_video_predictions=[]\n",
        "  if not video_capture.isOpened():\n",
        "    return('Could not open video')\n",
        "  valid_input_voice=check_audio_in_video(input_video)\n",
        "  if valid_input_voice == False:\n",
        "     return (\"the input video should have audio\")\n",
        "  audio_file = \"extracted_audio.wav\"\n",
        "  extract_audio_from_video(input_video, audio_file)\n",
        "  segments = split_and_pad_audio(audio_file, segment_duration=10, sample_rate=16000)\n",
        "  mfcc_data_df = extract_mfcc_from_segments(segments, sample_rate=16000, n_mfcc=13)\n",
        "  mfcc_data_df.drop(['segment_index'], axis=1, inplace=True)\n",
        "  scaler = load('scaler.joblib')\n",
        "  mfcc_data_df = scaler.transform(mfcc_data_df)\n",
        "  mfcc_data_reshaped = np.expand_dims(mfcc_data_df, axis=1)\n",
        "  y_pred = model.predict(mfcc_data_reshaped)\n",
        "  voice_predictions = np.argmax(y_pred, axis=1)\n",
        "  voice_predictions = 1 - voice_predictions\n",
        "  replicated_voice_predictions = np.repeat(voice_predictions, 10, axis=0)\n",
        "  return replicated_voice_predictions\n"
      ],
      "metadata": {
        "id": "4WC_Y9CIANN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install flask flask-ngrok"
      ],
      "metadata": {
        "collapsed": true,
        "id": "lZsibZe1pXuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install flask-ngrok\n",
        "# !apt-get install -y ngrok"
      ],
      "metadata": {
        "id": "vJFcgDceqgsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pyngrok"
      ],
      "metadata": {
        "id": "d1d0D9P9rU64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **voting**"
      ],
      "metadata": {
        "id": "ZLZb276e8XZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !ngrok authtoken YOUR_AUTHTOKEN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6ac16Rgre8_",
        "outputId": "e2044ad1-bedd-4aba-86a0-a323793998d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from flask import Flask, request, jsonify\n",
        "# from flask_ngrok import run_with_ngrok\n",
        "# import numpy as np\n",
        "\n",
        "# app = Flask(__name__)\n",
        "# run_with_ngrok(app)  # This will open an ngrok tunnel\n",
        "\n",
        "# latest_result = None  # متغير لتخزين النتيجة الأخيرة\n",
        "\n",
        "# def make_prediction_audio(input_video):\n",
        "#     return np.random.randint(0, 2, size=(len(input_video) // 10,)).tolist()\n",
        "\n",
        "# def make_prediction(input_video):\n",
        "#     return np.random.randint(0, 2, size=(len(input_video), 1)).tolist()\n",
        "\n",
        "# def final_decision(input_video):\n",
        "#     global latest_result\n",
        "\n",
        "#     voice_predictions = make_prediction_audio(input_video)\n",
        "#     facial_predictions = make_prediction(input_video)\n",
        "\n",
        "#     predictions_combined = np.vstack(facial_predictions)\n",
        "#     facial_predictions = np.argmax(predictions_combined, axis=1)\n",
        "\n",
        "#     replicated_voice_predictions = np.repeat(voice_predictions, 10, axis=0)\n",
        "#     replicated_voice_predictions = replicated_voice_predictions[:len(facial_predictions)]\n",
        "\n",
        "#     hard_vote = (np.array(replicated_voice_predictions) + np.array(facial_predictions)) >= 1\n",
        "#     hard_vote = hard_vote.astype(int)\n",
        "\n",
        "#     lie = np.sum(hard_vote)\n",
        "#     truth = len(hard_vote) - lie\n",
        "#     confidence = (max(lie, truth) / len(hard_vote)) * 100\n",
        "#     final_decision = \"Lie\" if lie > truth else \"Truth\"\n",
        "\n",
        "#     latest_result = {\n",
        "#         \"voice_predictions\": replicated_voice_predictions.tolist(),\n",
        "#         \"facial_predictions\": facial_predictions.tolist(),\n",
        "#         \"hard_vote\": hard_vote.tolist(),\n",
        "#         \"final_decision\": final_decision,\n",
        "#         \"confidence\": confidence\n",
        "#     }\n",
        "#     return latest_result\n",
        "\n",
        "# @app.route(\"/analyze\", methods=[\"POST\"])\n",
        "# def analyze():\n",
        "#     data = request.json\n",
        "#     input_video = data.get(\"video\", [])  # استلام بيانات الفيديو كقائمة\n",
        "#     if not input_video:\n",
        "#         return jsonify({\"error\": \"No video data provided\"}), 400\n",
        "\n",
        "#     result = final_decision(input_video)\n",
        "#     return jsonify(result)\n",
        "\n",
        "# @app.route(\"/result\", methods=[\"GET\"])\n",
        "# def get_result():\n",
        "#     if latest_result is None:\n",
        "#         return jsonify({\"error\": \"No analysis has been performed yet\"}), 404\n",
        "#     return jsonify(latest_result)\n",
        "\n",
        "# # Run the app with debug mode set in app.config instead of passing it directly to app.run()\n",
        "# if __name__ == \"__main__\":\n",
        "#     app.config[\"DEBUG\"] = True\n",
        "#     app.run()\n"
      ],
      "metadata": {
        "id": "BF632bzZxMi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from flask import Flask, request, jsonify\n",
        "# import numpy as np\n",
        "\n",
        "# app = Flask(__name__)\n",
        "\n",
        "# latest_result = None\n",
        "\n",
        "# def make_prediction_audio(input_video):\n",
        "#     return np.random.randint(0, 2, size=(len(input_video) // 10,)).tolist()\n",
        "\n",
        "# def make_prediction(input_video):\n",
        "#     return np.random.randint(0, 2, size=(len(input_video), 1)).tolist()\n",
        "\n",
        "# def final_decision(input_video):\n",
        "#     global latest_result\n",
        "\n",
        "#     voice_predictions = make_prediction_audio(input_video)\n",
        "#     facial_predictions = make_prediction(input_video)\n",
        "\n",
        "#     predictions_combined = np.vstack(facial_predictions)\n",
        "#     facial_predictions = np.argmax(predictions_combined, axis=1)\n",
        "\n",
        "#     replicated_voice_predictions = np.repeat(voice_predictions, 10, axis=0)\n",
        "#     replicated_voice_predictions = replicated_voice_predictions[:len(facial_predictions)]\n",
        "\n",
        "#     hard_vote = (np.array(replicated_voice_predictions) + np.array(facial_predictions)) >= 1\n",
        "#     hard_vote = hard_vote.astype(int)\n",
        "\n",
        "#     lie = np.sum(hard_vote)\n",
        "#     truth = len(hard_vote) - lie\n",
        "#     confidence = (max(lie, truth) / len(hard_vote)) * 100\n",
        "#     final_decision = \"Lie\" if lie > truth else \"Truth\"\n",
        "\n",
        "#     latest_result = {\n",
        "#         \"voice_predictions\": replicated_voice_predictions.tolist(),\n",
        "#         \"facial_predictions\": facial_predictions.tolist(),\n",
        "#         \"hard_vote\": hard_vote.tolist(),\n",
        "#         \"final_decision\": final_decision,\n",
        "#         \"confidence\": confidence\n",
        "#     }\n",
        "#     return latest_result\n",
        "\n",
        "# @app.route(\"/analyze\", methods=[\"POST\"])\n",
        "# def analyze():\n",
        "#     data = request.files\n",
        "#     input_video = data.get(\"video\", [])\n",
        "#     if not input_video:\n",
        "#         return jsonify({\"error\": \"No video data provided\"}), 400\n",
        "\n",
        "#     result = final_decision(input_video)\n",
        "#     return jsonify(result)\n",
        "\n",
        "# @app.route(\"/result\", methods=[\"GET\"])\n",
        "# def get_result():\n",
        "#     if latest_result is None:\n",
        "#         return jsonify({\"error\": \"No analysis has been performed yet\"}), 404\n",
        "#     return jsonify(latest_result)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     app.run(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoTzljm4yDpt",
        "outputId": "6cbdd01f-7567-4695-c808-c28b49c42e6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug: * Restarting with stat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from flask import Flask, request, jsonify\n",
        "# import numpy as np\n",
        "# import os\n",
        "\n",
        "# app = Flask(__name__)\n",
        "\n",
        "# # Set upload folder\n",
        "# UPLOAD_FOLDER = \"uploads\"\n",
        "# os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
        "# app.config[\"UPLOAD_FOLDER\"] = UPLOAD_FOLDER\n",
        "\n",
        "# latest_result = None\n",
        "\n",
        "# def make_prediction_audio(input_video):\n",
        "#     return np.random.randint(0, 2, size=(len(input_video) // 10,)).tolist()\n",
        "\n",
        "# def make_prediction(input_video):\n",
        "#     return np.random.randint(0, 2, size=(len(input_video), 1)).tolist()\n",
        "\n",
        "# def final_decision(video_path):\n",
        "#     global latest_result\n",
        "\n",
        "#     # Simulate reading video file as binary\n",
        "#     with open(video_path, \"rb\") as f:\n",
        "#         input_video = f.read()\n",
        "\n",
        "#     # Fake predictions\n",
        "#     voice_predictions = make_prediction_audio(input_video)\n",
        "#     facial_predictions = make_prediction(input_video)\n",
        "\n",
        "#     predictions_combined = np.vstack(facial_predictions)\n",
        "#     facial_predictions = np.argmax(predictions_combined, axis=1)\n",
        "\n",
        "#     replicated_voice_predictions = np.repeat(voice_predictions, 10, axis=0)\n",
        "#     replicated_voice_predictions = replicated_voice_predictions[:len(facial_predictions)]\n",
        "\n",
        "#     hard_vote = (np.array(replicated_voice_predictions) + np.array(facial_predictions)) >= 1\n",
        "#     hard_vote = hard_vote.astype(int)\n",
        "\n",
        "#     lie = np.sum(hard_vote)\n",
        "#     truth = len(hard_vote) - lie\n",
        "#     confidence = (max(lie, truth) / len(hard_vote)) * 100\n",
        "#     final_decision = \"Lie\" if lie > truth else \"Truth\"\n",
        "\n",
        "#     latest_result = {\n",
        "#         \"voice_predictions\": replicated_voice_predictions.tolist(),\n",
        "#         \"facial_predictions\": facial_predictions.tolist(),\n",
        "#         \"hard_vote\": hard_vote.tolist(),\n",
        "#         \"final_decision\": final_decision,\n",
        "#         \"confidence\": confidence\n",
        "#     }\n",
        "#     return latest_result\n",
        "# @app.route(\"/analyze\", methods=[\"POST\"])\n",
        "# def analyze():\n",
        "#     print(\"Received request with Content-Type:\", request.content_type)\n",
        "#     print(\"Request files:\", request.files)\n",
        "\n",
        "#     if \"video\" not in request.files:\n",
        "#         return jsonify({\"error\": \"No video file provided\"}), 400\n",
        "\n",
        "#     video_file = request.files[\"video\"]\n",
        "\n",
        "#     if video_file.filename == \"\":\n",
        "#         return jsonify({\"error\": \"No selected file\"}), 400\n",
        "\n",
        "#     file_path = os.path.join(app.config[\"UPLOAD_FOLDER\"], video_file.filename)\n",
        "#     video_file.save(file_path)\n",
        "\n",
        "#     result = final_decision(file_path)\n",
        "#     return jsonify(result)\n",
        "\n",
        "\n",
        "# @app.route(\"/result\", methods=[\"GET\"])\n",
        "# def get_result():\n",
        "#     if latest_result is None:\n",
        "#         return jsonify({\"error\": \"No analysis has been performed yet\"}), 404\n",
        "#     return jsonify(latest_result)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     app.run(host=\"0.0.0.0\", port=5000, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qmHcGqVD7CE",
        "outputId": "25ced691-6c18-44a8-a715-58602c1df029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug: * Restarting with stat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from flask import Flask, request, jsonify\n",
        "# import numpy as np\n",
        "# import os\n",
        "\n",
        "# app = Flask(__name__)\n",
        "\n",
        "# # Set upload folder\n",
        "# UPLOAD_FOLDER = \"uploads\"\n",
        "# os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
        "# app.config[\"UPLOAD_FOLDER\"] = UPLOAD_FOLDER\n",
        "\n",
        "# latest_result = None\n",
        "\n",
        "# def make_prediction_audio(input_video):\n",
        "#     return np.random.randint(0, 2, size=(len(input_video) // 10,)).tolist()\n",
        "\n",
        "# def make_prediction(input_video):\n",
        "#     return np.random.randint(0, 2, size=(len(input_video), 1)).tolist()\n",
        "\n",
        "# def final_decision(video_path):\n",
        "#     global latest_result\n",
        "\n",
        "#     if not os.path.exists(video_path):\n",
        "#         return {\"error\": \"File not found\"}\n",
        "\n",
        "#     print(f\"Processing file: {video_path}\")\n",
        "\n",
        "#     with open(video_path, \"rb\") as f:\n",
        "#         input_video = f.read()\n",
        "\n",
        "#     if not input_video:\n",
        "#         return {\"error\": \"File is empty or unreadable\"}\n",
        "\n",
        "#     # Fake predictions\n",
        "#     voice_predictions = make_prediction_audio(input_video)\n",
        "#     facial_predictions = make_prediction(input_video)\n",
        "\n",
        "#     facial_predictions = np.vstack(np.array(facial_predictions))  # Ensure correct format\n",
        "#     facial_predictions = np.argmax(facial_predictions, axis=1)\n",
        "\n",
        "#     replicated_voice_predictions = np.repeat(voice_predictions, 10, axis=0)\n",
        "#     replicated_voice_predictions = replicated_voice_predictions[:len(facial_predictions)]\n",
        "\n",
        "#     hard_vote = (np.array(replicated_voice_predictions) + np.array(facial_predictions)) >= 1\n",
        "#     hard_vote = hard_vote.astype(int)\n",
        "\n",
        "#     lie = np.sum(hard_vote)\n",
        "#     truth = len(hard_vote) - lie\n",
        "#     confidence = (max(lie, truth) / len(hard_vote)) * 100\n",
        "#     final_decision = \"Lie\" if lie > truth else \"Truth\"\n",
        "\n",
        "#     latest_result = {\n",
        "#         \"voice_predictions\": replicated_voice_predictions.tolist(),\n",
        "#         \"facial_predictions\": facial_predictions.tolist(),\n",
        "#         \"hard_vote\": hard_vote.tolist(),\n",
        "#         \"final_decision\": final_decision,\n",
        "#         \"confidence\": confidence\n",
        "#     }\n",
        "#     return latest_result\n",
        "\n",
        "# @app.route(\"/analyze\", methods=[\"POST\"])\n",
        "# def analyze():\n",
        "#     print(\"Received request with Content-Type:\", request.content_type)\n",
        "#     print(\"Request files:\", request.files)\n",
        "\n",
        "#     if \"video\" not in request.files:\n",
        "#         return jsonify({\"error\": \"No video file provided\"}), 400\n",
        "\n",
        "#     video_file = request.files[\"video\"]\n",
        "\n",
        "#     if video_file.filename == \"\":\n",
        "#         return jsonify({\"error\": \"No selected file\"}), 400\n",
        "\n",
        "#     file_path = os.path.join(app.config[\"UPLOAD_FOLDER\"], video_file.filename)\n",
        "#     video_file.save(file_path)\n",
        "\n",
        "#     result = final_decision(file_path)\n",
        "#     return jsonify(result)\n",
        "\n",
        "# @app.route(\"/result\", methods=[\"GET\"])\n",
        "# def get_result():\n",
        "#     if latest_result is None:\n",
        "#         return jsonify({\"error\": \"No analysis has been performed yet\"}), 404\n",
        "#     return jsonify(latest_result)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     app.run(host=\"0.0.0.0\", port=5000, debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXqwNcplMyzm",
        "outputId": "c8a5ea89-eb2e-4088-ecac-985ceed4e616"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug: * Restarting with stat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !ngrok http 5000\n"
      ],
      "metadata": {
        "id": "SJCeElL7AN_S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2243c60a-6690-47b4-f84e-da8a545ed998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR:  authentication failed: The authtoken you specified does not look like a proper ngrok tunnel authtoken.\n",
            "ERROR:  Your authtoken: YOUR_AUTHTOKEN\n",
            "ERROR:  Instructions to install your authtoken are on your ngrok dashboard:\n",
            "ERROR:  https://dashboard.ngrok.com/get-started/your-authtoken\r\n",
            "ERROR:  \r\n",
            "ERROR:  ERR_NGROK_105\r\n",
            "ERROR:  https://ngrok.com/docs/errors/err_ngrok_105\r\n",
            "ERROR:  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dhyLyu38r8dx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}